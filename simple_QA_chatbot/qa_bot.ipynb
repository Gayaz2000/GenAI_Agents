{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d262ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "# api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "# api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1393f4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "class GroqLLM:\n",
    "    def __init__(self, model_name: str=\"llama3-8b-8192\"):\n",
    "        self.model_name = model_name\n",
    "        self.llm = None\n",
    "\n",
    "    def load_groq_llm(self, temperature: float = 0, max_tokens: int =1000):\n",
    "        \"\"\"loads required groq model\"\"\"\n",
    "        try:\n",
    "            if \"llama\" in self.model_name:\n",
    "                self.llm = ChatGroq(model=\"llama3-70b-8192\")\n",
    "                #return llm\n",
    "            elif \"deepseek\" in self.model_name:\n",
    "                self.llm = ChatGroq(model=\"deepseek-r1-distill-llama-70b\")\n",
    "                #return llm\n",
    "            elif \"gemma\" in self.model_name:\n",
    "                self.llm = ChatGroq(model=\"gemma2-9b-it\")\n",
    "                #return llm\n",
    "            elif \"qwen\" in self.model_name:\n",
    "                self.llm = ChatGroq(model=\"qwen-qwq-32b\")\n",
    "            else:\n",
    "                raise f\"model name was not given\"\n",
    "            return  self.llm\n",
    "        except Exception as e:\n",
    "            raise f\"Error occured as: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "323595fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "class QAChatbot:\n",
    "    def __init__(self):\n",
    "        self.llm = GroqLLM().load_groq_llm() # Enter the model name you want to load\n",
    "\n",
    "    def create_chain(self):\n",
    "        \"\"\"creates a chain using prompt and llm\"\"\"\n",
    "        template = \"\"\"\n",
    "            You are an helpful assistant. Your task is to provide answer to the user question as good as possible.\n",
    "            Question: {question}\n",
    "            Make the answer concise nad only provide correct and relevant response\n",
    "        \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "        chain = prompt | self.llm\n",
    "        #response = await chain.ainvoke({\"question\": question})\n",
    "        return  chain #response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1689b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_response(question):\n",
    "    \"\"\"returns response for user query\"\"\"\n",
    "    qa_obj = QAChatbot()\n",
    "    chain = qa_obj.create_chain()\n",
    "    response =await chain.ainvoke({\"question\": question})\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5064411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI stands for Artificial Intelligence, which refers to the development of computer systems that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "* Learning\n",
      "* Problem-solving\n",
      "* Reasoning\n",
      "* Perception\n",
      "* Understanding language\n",
      "\n",
      "AI systems use algorithms and data to make decisions, classify objects, recognize patterns, and generate insights, often with greater speed and accuracy than humans. The goal of AI is to create machines that can think, learn, and act like humans, but not necessarily in the same way.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    question = input(\"Enter your query: \")\n",
    "    #qa_obj = QAChatbot()\n",
    "    print(asyncio.run(get_response(question)))\n",
    "    #print(get_response(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2ee27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
